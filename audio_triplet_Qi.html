<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Audio-triplet-network</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/sub.css" />
</head>
<body>
<div id="body" class="container">
<div class="main">
<div class="entry" id="main">
<!-- content -->
<h1>Audio feature learning with triplet-based embedding network</h1>
<h2>Motivation</h2>
    <p>  
        Existing methods on audio version identification use hand-crafted features, which requires extensive human efforts and experts participation. We propose a triplet-based network for version identification, learning feature representation for a  music automatically.  Triplet-based neural network can learn segment-level features from training data, focusing on the most similar parts between music versions, thereby identical version of a music is detected easily. Extensive experiments demonstrate the effectiveness of our method.
    </p>
<h2>Background</h2>
    <p>  
        A version of a music is a new performance of a previous work. Given a query audio piece, version identification aims to retrieve all the versions in a database. Music version identification has wide application, especially in music copyright monitoring, while it remains challenging due to the variances in tempo, key, instruments, etc. Previous work mainly focus on extracting features from pitch class profiles or chroma, which well capture the tonal content of a music, and have been proved to be more effective than original audio spectrums descriptors such as Mel-frequency cepstrum coefficients.
    </p>
    <p>  
        Most version identification algorithms use some form of dynamic time-warping(DTW)(Serra 2011). Thierry et al.(Bertin-Mahieux and Ellis 2011) extracted hash code called ’jump code’ from beat-aligned chroma. Balen et al. (van Balen et al. 2014) proposed cognition-inspired descriptors as audio features. From then on, more methods such as information theoretic measures have been proposed with little performance improvement. Having seen the glass-ceiling for hand-crafted features, we thus turn to machine learning algorithms for higher learning capability to bridge the semantic gap in the long term.
    </p>
    <p>  
        Version identification is basically a problem of similarity metric learning. To this end, our goal is to embed the data to a feature space where versions of the same music are close to each other and music from different versions stay relatively far away. We propose a triplet-based neural network, which learns audio similarity from triplet input containing a query audio piece, a same version and a different version. In addition, in order to match music accurately, we conduct the learning process on segment-level instead of song-level. The experiment results demonstrate that our method outperforms hand-crafted ones. Our contribution can be summarized into two-folds. 1) To our knowledge, this is the first time that a triplet deep learning method has been applied to audio metric learning. 2) We propose an end-to-end framework and the triplet-based network shall be applicable for large-scale commercial use due to its high efficiency and great potential as the amount of audio data grows.
    </p>
<h2>Method formulation</h2>
    <!--l. 71-->
    <p class="noindent" >
        Given a training set <span class="cmsy-10"><img src="Images/cmsy10-44.png" alt="D" class="10x-x-44" /> </span><span class="cmr-10">= </span><span class="cmsy-10">{</span><span class="cmr-10">(</span><span class="cmmi-10">a</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,v</span><sub><span class="cmmi-7">i</span></sub><span class="cmr-10">)</span><span class="cmsy-10">}</span><sub><span class="cmmi-7">i</span><span class="cmr-7">=1</span></sub><sup><span class="cmmi-7">N</span></sup>, where <span class="cmmi-10">a</span><sub><span class="cmmi-7">i</span></sub> <span class="cmr-10">= </span><span class="cmsy-10">{</span><span class="cmmi-10">e</span><sub><span class="cmmi-7">j,k</span></sub><span class="cmsy-10">}</span><sub><span class="cmmi-7">l</span><span class="cmsy-7">&#x00D7;</span><span class="cmmi-7">d</span></sub>is an audio track, and <span class="cmmi-10">v</span><sub><span class="cmmi-7">i</span></sub> denotes the version it belongs to.<span class="cmmi-10">e</span><sub><span class="cmmi-7">j,k</span></sub> <span class="cmsy-10">&#x2208; </span><span class="cmr-10">(0</span><span class="cmmi-10">,</span><span class="cmr-10">1) </span>indicates audio energy of different frequency bands.<span class="cmmi-10">l </span>is audio duration and <span class="cmmi-10">d </span>is the number of frequency bands. Our goal is to learn a similarity metric <span class="cmmi-10">S</span><span class="cmr-10">(</span><span class="cmsy-10">&#x22C5;</span><span class="cmmi-10">,</span><span class="cmsy-10">&#x22C5;</span><span class="cmr-10">) </span>with <span class="cmsy-10"><img src="Images/cmsy10-44.png" alt="D" class="10x-x-44" /> </span>in order to makea prediction of the similarity degree given a new music pair<span class="cmr-10">(</span><span class="cmmi-10">a</span><sub><span class="cmmi-7">j</span></sub><span class="cmmi-10">,a</span><sub><span class="cmmi-7">k</span></sub><span class="cmr-10">)</span>.
     <!--l. 73-->
     <p class="indent" >    
        It is impractical to train a classifier for each kind of version since the amount of music of a version is relatively small and the version amount keeps increasing with new music being created. To solve this problem, we learn by distance comparison within triplets. Furthermore, we experiment on segment-level for more precise learning.
     <!--l. 75-->
     <p class="indent" >    
        Our framework includes two parts. Firstly, we construct triplets set <img src="Images/aaai1x.png" alt="&#x02DC;D"  class="tilde" > from <span class="cmsy-10"><img src="Images/cmsy10-44.png" alt="D" class="10x-x-44" /> </span>on segment-level. <img src="Images/aaai2x.png" alt="&#x02DC;D"  class="tilde" > <span class="cmr-10">= </span><span class="cmsy-10">{</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup><span class="cmr-10">)</span><span class="cmsy-10">}</span><sub><span class="cmmi-7">i</span><span class="cmr-7">=1</span></sub><sup><span class="cmmi-7">N</span></sup>,where <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> is a music segment, and <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup> denotes a same version of<span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub>, while <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup> represents a non-version one. Then, we embed audio with a triplet-based network and form the similarity metric on the embedded feature.
<h3>Audio segmentation and triplets generation</h3>
    <p>  
        We conduct audio segmentation and construct triplets set. We experiment on segment-level for two reasons, focusing on the most significant music part and solving the problem that audio versions of different genres usually vary in length while triplets should have equal length. We encode audio tracks and build an index for efficient segment retrieval to construct triplets.
    </p>
    <p> 
        1. A song is originally represented as a sequence of beat-aligned 12-dimensional chroma vectors. For easy indexing and computational simplicity, chroma vectors are encoded into binary codes, in a way that the dissimilarity between two original vectors is well approximated by the Hamming distance between the two codes. Concretely, the code of a vector contains two parts. One part concerns the vector itself, consisting of four 12-bit sub-codes. In each sub-code, the bits corresponding to the top m largest bins of the vector are set to 1 and others 0, with m being 6, 5, 3 and 1 respectively. The other part of a code represents the average of the neighbors of the vector. The average vector is encoded in a similar way but with a lower resolution. As a result, a chroma vector is represented by a 64-bit binary code.
    </p>
    <p>  
        2. Create a music version dataset D={<span class="cmmi-10">x</span><sub><span class="cmmi-7">1</span></sub>,<span class="cmmi-10">x</span><sub><span class="cmmi-7">2</span></sub>,…,<span class="cmmi-10">x</span><sub><span class="cmmi-7">N</span></sub> }, in which song <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> (i∈[1..N]) is represented by a sequence of 64-bit codes, and build a codes index which indicates the occurrences of each unique code in all the songs in D. The codes index is only based on the first parts of codes, without considering the code part for the average vector.
    </p>
    <p>  
        3. Given a pair of songs (<span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup>) in D, which are versions of the same song, and <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> being the query song, divide <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> into segments of length l, with step size d.
    </p>
    <p> 
        4. For each segment of <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub>, find its nearest one in <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup>. The details of the process are as follows. For each code in a <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> segment, find out through the index all occurrences in <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup> of the code (and its transpositions), and then, for each occurrence, calculate the Hamming distance between the <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub> segment and the occurrence-aligned <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup> segment, keeping the order of codes when matching codes between the segments.
    </p>
    <p>  
        5. Generate pairs of similar chroma segments between <span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup>. Only segment pairs with a similarity larger than the threshold u will be selected. For each similar segment pair, randomly select a chroma segment from a different song, to make a chroma segment triplet.
    </p>
    <p>  
        6. Return the generated chroma segment triplets.
    </p>
    
<h3>Triplet-based network feature learning</h3>
    We propose a triplet-based embedding neural network for audio metric learning. Given an audio triplet <span class="cmr-10">(</span><span class"cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup><span class="cmr-10">)</span>, triplet-based neural network embeds them to a vector space, generating<span class="cmmi-10">f</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmr-10">)</span><span class="cmmi-10">,f</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup><span class "cmr-10">) </span>and <span class="cmmi-10">f</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup><span class="cmr-10">) </span>respectively. Specifically, as we show in Figure 1, <span class="cmmi-10">f</span><span class="cmr-10">(</span><span class="cmsy-10">&#x22C5;</span><span class="cmr-10">) </span>consists of two convolution layers, with Relu as activation function, followed by two max-pooling layers and two full connection layers. With the audio embeddings, we adopt the online distance metric learning algorithm with cosine similarity proposed in (Wu et al). The similarity function is denoted as follows:
        <table class="equation"><tr><td><a id="x1-2002r1"></a> <center class="math-display" ><img src="Images/aaai5x.png" alt="S(xi,xj) = f (xi)Tf(xj)&#x2215;(||f(xi)||&#x00D7; ||f(xj)||)" class="math-display" ></center></td><td class="equation-label">(1)</td></tr></table>
    The hinge loss (Wan et al) is defined as:
        <table class="equation"><tr><td><a id="x1-2003r2"></a><center class="math-display" ><img src="Images/aaai7x.png" alt="   N&#x2211;  +       -L (D; &#x03B8;) =   max (1- S(xi,xi )+ S(xi,xi ),0) i=1" class="math-display" ></center></td><td class="equation-label">(2)</td></tr></table>
    where <span class="cmmi-10">&#x03B8; </span>represents all of the parameters in our model, and <span class="cmmi-10">N</span> denotes the number of training instances. <span class="cmmi-10">&#x03B8; </span>is updated by back-propagation and each base network in the triplet network share the same architecture and parameter weights.
        <hr class="figure">
            <div class="figure" > <a id="x1-2001r1"></a>&#x00A0;<img src="Images/framework.jpg" align="center" alt="PIC" class="graphics"<!--tex4ht:graphics name="aaai3x.png" src="framework.pdf"  --><br /> <div class="caption" ><span class="id">Figure&#x00A0;1: </span><span  class="content">Triplet network</span></div><!--tex4ht:label?: x1-2001r1 -->&#x00A0;
            </div>
        <hr class="endfigure">
<h2>Experiment</h2>
    <!--l. 102-->
    <p class="noindent" >
        We use the SecondHandSongs dataset (SHSD) which is an official list of cover songs within the Million Song Dataset (MSD) released in 2011. SHSD is the largest public cover song dataset up to now, including 18196 tracks in 5854 cliques. To build our model, we use the training split of SHSD dataset with 11460 tracks and test on the standard Query1500 testing set(including 500 triplets).
    <!--l. 104-->
    <p class="indent" >    
        We clip the training audios into segments and experiment on length from 100 to 700 with a step of 50. We randomly sampled 1,000,000 triplets for training and 100,000 for testing, <span class="cmr-10">10 </span>to <span class="cmr-10">100</span> times of our network parameter number complexity which is <span class="cmmi-10">O</span><span class="cmr-10">(10</span><sup><span class="cmr-7">4</span></sup><span class="cmr-10">)</span>.
    <!--l. 106-->
    <p class="indent" >    
        We experiment on various parameter weights and set the convolution kernel size to 3, pooling size and stride both to 2 forbest performance. The first full connection layer has 100 nodes and the second has 50 nodes as output. The triplet network is implemented with Caffe.       
    <!--l. 108-->
    <p class="indent" >    
        We test the model on the Query1500 data set which is given in the form of <span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup><span class="cmr-10">)</span>, first two of the same version and the third different from the first. We calculate querying accuracy by comparing the similarity function results <span class="cmmi-10">S</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmr-7">+</span></sup><span class="cmr-10">) </span>and<span class="cmmi-10">S</span><span class="cmr-10">(</span><span class="cmmi-10">x</span><sub><span class="cmmi-7">i</span></sub><span class="cmmi-10">,x</span><sub><span class="cmmi-7">i</span></sub><sup><span class="cmsy-7">-</span></sup><span class="cmr-10">)</span>.
            <table class="equation"><tr><td><a id="x1-3001r3"></a><center class="math-display" ><img src="Images/aaai8x.png" alt="        T&#x2211;Acc = 1-   g(S (xi,x+i )- S (xi,x-i ))T i=1" class="math-display" ></center></td><td class="equation-label">(3)</td></tr></table>
     <!--l. 111-->
     <p class="nopar" >
        where <span class="cmmi-10">g</span><span class="cmr-10">(</span><span class="cmmi-10">c</span><span class="cmr-10">) = 1 </span>if <span class="cmmi-10">c &#x003C; </span><span class="cmr-10">0 </span>and <span class="cmmi-10">g</span><span class="cmr-10">(</span><span class="cmmi-10">c</span><span class="cmr-10">) = 0 </span>otherwise. <span class="cmmi-10">T </span>denotes the number of testing instances. The experiment results are listed in Table 1.
        <div class="table">
        <!--l. 114-->
        <p class="indent" >  <a id="x1-3002r1"></a><hr class="float"><div class="float" ><a id="Q1-1-6"></a>
        <table border="1px" bordercolor="#000000" cellspacing="0px" style="border-collapse:collapse" align="center">
        <tr><th>Method</th><th>Jump codes</th><th>DTW</th><th>Cognition</th><th>Triplet</th>
        <tr><td>Acc</td><td>77.4%</td><td>80.0%</td><td>73.2%</td><td>81.2%</td>
        </table>
<div class="row">
<h2>Reference</h2>
<div class="col-sm-12">
    <dl class="dl-horizontal">
        <dt>bertin2012large</dt>
        <dd>Bertin-Mahieux, T., and Ellis, D. P. 2011. Large-scale cover song recognition using hashed chroma landmarks. In 2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 117–120. IEEE. 
        </dd>
    </dl>
    <dl class="dl-horizontal">
        <dt>serra2011identification</dt>
        <dd>Serra, J. 2011. Identification of versions of the same musical composition by processing audio descriptions. Department of Information and Communication Technologies.
        </dd>
    </dl>
    <dl class="dl-horizontal">
        <dt>van2014cognition</dt>
        <dd>van Balen, J.; Bountouridis, D.; Wiering, F.; Veltkamp, R. C.; et al. 2014. Cognition-inspired descriptors for scalable cover song retrieval. In proceedings of the 15th international conference on Music Information Retrieval.
        </dd>
    </dl>
    <dl class="dl-horizontal">
        <dt>wan2014deep</dt>
        <dd>Wan, J.; Wang, D.; Hoi, S. C. H.; Wu, P.; Zhu, J.; Zhang, Y.; and Li, J. 2014. Deep learning for content-based image retrieval: A comprehensive study. In Proceedings of the 22nd ACM international conference on Multimedia, 157– 166. ACM.
        </dd>
    </dl>
    <dl class="dl-horizontal">
        <dt>wu2013online</dt>
        <dd>Wu, P.; Hoi, S. C.; Xia, H.; Zhao, P.;Wang, D.; and Miao, C. 2013. Online multimodal deep similarity learning with application to image retrieval. In Proceedings of the 21st ACM international conference on Multimedia, 153–162. ACM.
        </dd>
    </dl>
    <dl class="dl-horizontal">
        <dt></dt>
        <dd></dd>
    </dl>
</div>
<!-- content end -->
</div>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>Edit by Xiaoyu Qi 2016</p>
	</div>
</div>
<script src="main.js"></script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</div>
</body>
</html>
